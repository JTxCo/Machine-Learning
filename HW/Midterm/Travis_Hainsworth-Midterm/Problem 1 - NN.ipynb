{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a dataset on Kaggle that you find interesting, put the \n",
    "# data in the in the correct directory, and include a hyperlink\n",
    "# to it here:\n",
    "# hyperlink: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams\n",
    "\n",
    "\n",
    "\n",
    "# Packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0       1        group B           bachelor's degree      standard   \n",
       "1       1        group C                some college      standard   \n",
       "2       1        group B             master's degree      standard   \n",
       "3       0        group A          associate's degree  free/reduced   \n",
       "4       0        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### -------- Import Data and Data Preprocessing -------- ###\n",
    "# you must include the appropriate data preprocessing steps\n",
    "# dataset is in /Training\\ Data/1-NN_Data/StudentsPerformance.csv\n",
    "'''\n",
    "\"gender\",\"race/ethnicity\",\"parental level of education\",\"lunch\",\"test preparation course\",\"math score\",\"reading score\",\"writing score\"\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('Training Data/1-NN_Data/StudentsPerformance.csv')\n",
    "# Convert female to 1, male to 0\n",
    "dataset[\"gender\"] = [1 if each == \"female\" else 0 for each in dataset[\"gender\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lunch\n",
       "free/reduced    63.022535\n",
       "standard        70.823256\n",
       "Name: writing score, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ------------ Exploratory Data Analysis ------------- ###\n",
    "# Output two pieces of information that you found \n",
    "# informative as well as a print statement of why they\n",
    "# assisted you in choosing your model parameters\n",
    "# race_score = dataset.groupby(\"race/ethnicity\")[\"math score\"].mean()\n",
    "# race_score = dataset.groupby(\"race/ethnicity\")[\"writing score\"].mean()\n",
    "race_score = dataset.groupby(\"lunch\")[\"math score\"].mean()\n",
    "race_score = dataset.groupby(\"lunch\")[\"writing score\"].mean()\n",
    "race_score\n",
    "# I thought this was just interesting to see the average writing for each race and how they compare\n",
    "# I also saw how important lunch was to the math score especially, so I thought it would be interesting to see how it affects the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 17)\n",
      "(800,)\n",
      "(200, 17)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Setting the X and Y\n",
    "# I am predicting whether or not they had a good lunch \n",
    "shuffle_dt = dataset.sample(frac=1)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_set = shuffle_dt[:train_size]\n",
    "test_set = shuffle_dt[train_size:]\n",
    "\n",
    "# Set up X_train\n",
    "X_train = train_set.drop(\"lunch\", axis=1)\n",
    "X_train = pd.get_dummies(X_train, columns=[\"race/ethnicity\",\"parental level of education\",\"test preparation course\"])\n",
    "# Set up Y_train\n",
    "Y_train = train_set[\"lunch\"]\n",
    "\n",
    "# Set up X_test and Y_test\n",
    "X_test = test_set.drop(\"lunch\", axis=1)\n",
    "X_test = pd.get_dummies(X_test, columns=[\"race/ethnicity\",\"parental level of education\",\"test preparation course\"])\n",
    "Y_test = test_set[\"lunch\"]\n",
    "\n",
    "# printing the shape\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------- Model Definition ------------------ ###\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, hidden_size1, hidden_size2):\n",
    "        super(ANN, self).__init__()\n",
    "        # Linear function 1: 18 to hidden_size1\n",
    "        self.linear1 = nn.Linear(18, hidden_size1)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "\n",
    "        # Linear function 2: hidden_size1 to hidden_size2\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "\n",
    "        # Linear function 3: hidden_size2 to 2 (output layer)\n",
    "        self.linear3 = nn.Linear(hidden_size2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.tanh1(out)\n",
    "\n",
    "        out = self.linear2(out)\n",
    "        out = self.tanh2(out)\n",
    "\n",
    "        out = self.linear3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I chose the Adam optimizer because it is a good optimizer for most problems and I chose the CrossEntropyLoss because it is good for classification problems\n"
     ]
    }
   ],
   "source": [
    "### --------- Optimizer and Loss Definition ------------ ###\n",
    "# Output a print statement supporting your optimizer and \n",
    "# loss function choices\n",
    "\n",
    "model = ANN()\n",
    "# print(list(model.parameters()))\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"I chose the Adam optimizer because it is a good optimizer for most problems and I chose the CrossEntropyLoss because it is good for classification problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ANN.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hidden_size \u001b[38;5;129;01min\u001b[39;00m hidden_sizes:\n\u001b[0;32m---> 22\u001b[0m         model \u001b[38;5;241m=\u001b[39m ANN(hidden_size)\n\u001b[1;32m     23\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# for simplicity let's assume binary classification\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ANN.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "### ---------------- Training pt I --------------------- ###\n",
    "# Using this cell and the next, tune a minimum of 2\n",
    "# hyperparameters\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "hidden_sizes = [10, 20, 30]\n",
    "\n",
    "best_lr = None\n",
    "best_hidden = None\n",
    "best_acc = 0\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# assuming criterion is defined somewhere else\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 20  # set any integer value appropriate for your data\n",
    "\n",
    "# Loop through the combinations\n",
    "for lr in learning_rates:\n",
    "    for hidden_size in hidden_sizes:\n",
    "        model = ANN(hidden_size)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # for simplicity let's assume binary classification\n",
    "        for epoch in range(n_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            for i, data in enumerate(trainloader):  # needs definition\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "        # Test the model\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # since we're testing, no need to compute gradients\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:  # needs definition\n",
    "                features, labels = data\n",
    "                \n",
    "                outputs = model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        acc = correct / total  # this is the accuracy\n",
    "\n",
    "        # compare with the best_acc and update the best hyperparameters if needed.\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_lr = lr\n",
    "            best_hidden = hidden_size\n",
    "\n",
    "print(\"Best learning rate: %s\" % best_lr)\n",
    "print(\"Best hidden size: %s\" % best_hidden)\n",
    "print(\"Best accuracy: %s\" % best_acc)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------- Validation pt I -------------------- ###\n",
    "# Print your initial accuracy, then your final accuracy \n",
    "# after tuning. Also print what hyperparameters you tuned\n",
    "# and which seemed to have the biggest impact on the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------- Hyperparameter Tuning ---------------- ###\n",
    "# Execute a 2d grid search with a minimum of 15 samples for\n",
    "# each parameter and output a surface plot of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------- Training pt II --------------------- ###\n",
    "# Re-train your model using your optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------- Validation pt II -------------------- ###\n",
    "# Print your accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------ Testing ------------------------- ###\n",
    "# Print your final accuracy and print some comments on how\n",
    "# it compares to your validation accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
